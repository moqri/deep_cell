{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough of code\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "Mustafa Hajij (did: NN architecture coding, evaluation metric coding)\n",
    "\n",
    "Sierra Lear (did: downsampling function, running models using different hyperparameters, compiled/cleaned code into this Jupyter notebook)\n",
    "\n",
    "Mahdi Moqri (did: all data collection and preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import savetxt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import InputLayer\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import keras #Not needed?\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myplot(x, y, s, bins=150):\n",
    "    heatmap, xedges, yedges = np.histogram2d(x, y, bins=bins)\n",
    "    heatmap = gaussian_filter(heatmap, sigma=s)\n",
    "    return heatmap.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_point_cloud_from_hd(path):\n",
    "    entiregraph=[]\n",
    "    y=[]\n",
    "    with open(path) as f:\n",
    "         for line in f:\n",
    "            numbers_str = line.split()\n",
    "            numbers_float = [str(x) for x in numbers_str]  #map(float,numbers_str) works too\n",
    "            val=float(numbers_float[-1])\n",
    "            if val!=2:          \n",
    "                y.append(val)\n",
    "                entiregraph.append(numbers_float[:-1])\n",
    "            else:\n",
    "                y.append(1.0)\n",
    "                entiregraph.append(numbers_float[:-1])\n",
    "    return np.array(entiregraph),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(expression_df,pairs,y_values,s=16,threshold=0.05,size=None):\n",
    "\n",
    "    expression_df = expression_df.loc[:,~expression_df.columns.duplicated()]\n",
    "    X=[]\n",
    "    y_out=[]\n",
    "    for pair,k in zip(pairs,y_values):\n",
    "        \n",
    "        if size!=None:\n",
    "            \n",
    "            if len(X)>size:\n",
    "                break\n",
    "        \n",
    "        gene_a=pair[0]  \n",
    "        gene_b=pair[1]\n",
    "        \n",
    "        if gene_a in expression_df and gene_b in expression_df : # check if the genes are in the table before plugging them in\n",
    "            d=expression_df[[gene_a,gene_b]]\n",
    "            d=d[d[gene_a]+d[gene_b]>threshold] # here we use the threshold \n",
    "        \n",
    "        \n",
    "            x=np.log2(d+1)[gene_a].values\n",
    "            y=np.log2(d+1)[gene_b].values  \n",
    "\n",
    "\n",
    "            \n",
    "            if len(x)!=0:\n",
    "                if len(x.shape)!=1:\n",
    "                    \n",
    "                    continue\n",
    "                else:\n",
    "                    \n",
    "                      \n",
    "                    img = myplot(x, y, s,bins=150) # create the image\n",
    "                    \n",
    "                    X.append(img)\n",
    "                    y_out.append(k)\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                    img = myplot(x, y, s,bins=150)  # create the image\n",
    "             \n",
    "\n",
    "                    X.append(img)\n",
    "                    y_out.append(k)\n",
    "              \n",
    "\n",
    "        \n",
    "    \n",
    "    return np.array(X),np.array(y_out)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download all the raw data--namely single-cell expression data from this URL: https://mousescexpression.s3.amazonaws.com/dendritic_cell.h5.\n",
    "\n",
    "In the cell below, I saved it as \"dendritic_cell.h5.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_proj_dir = os.path.join(os.path.expanduser('~'), 'Documents/GitHub/deep_cell_personal/dendritic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(my_proj_dir,'my_matrix.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= os.path.join(os.path.expanduser('~'), 'Documents/GitHub/deep_cell_personal/dendritic') #Note: must change to match location on your device\n",
    "#genes_url='https://raw.githubusercontent.com/moqri/deep_cell/master/dendritic/genes.txt'\n",
    "#labels_url='https://raw.githubusercontent.com/moqri/deep_cell/master/dendritic_gene_pairs_200.txt'\n",
    "\n",
    "genes = pd.read_table(os.path.join(folder,'genes.txt'),index_col=1,header=None,names=['gene', 'id'],delimiter=' ') #references genes names and their id in raw data\n",
    "\n",
    "expression_df = pd.read_hdf('~/Downloads/dendritic_cell.h5',index_col=0) #Note: this references/loads in the dendritic_cell raw data mentioned above. Again, location may have to be changed to match your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure genes downloaded correctly\n",
    "genes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure expression_df downloaded correctly\n",
    "expression_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data\n",
    "\n",
    "expression_df.index.rename('cell_id',inplace=1)\n",
    "expression_df.shape\n",
    "\n",
    "gene_names=[genes['gene'].loc[id] for id in expression_df.columns.values] #adding appropriate gene names. Note: used loc instead ix because ix doesn't work on my machine.\n",
    "expression_df.columns=gene_names \n",
    "\n",
    "expression_df.head()\n",
    "\n",
    "expression_df=expression_df.loc[:, (expression_df != expression_df.iloc[0]).any()]  # remove constant columns\n",
    "expression_df.shape\n",
    "\n",
    "expression_df=expression_df[(expression_df.T != 0).any()] # remove rows of zeros\n",
    "expression_df.shape\n",
    "\n",
    "\n",
    "gene_count=expression_df.astype(bool).sum(axis=0)\n",
    "cells=expression_df.count()\n",
    "expression_df=expression_df[gene_count[gene_count>cells/10].index]\n",
    "expression_df.shape\n",
    "\n",
    "expression_df=(100*expression_df.transpose() / expression_df.sum(1)).round(2).transpose()\n",
    "expression_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate labels for different gene pairs from text file\n",
    "labels= pd.read_table(folder+\"/dendritic_gene_pairs_200.txt\",index_col=False,header=None,names=['gene1', 'gene2','value'],delimiter='\\t')\n",
    "pairs,y_=load_point_cloud_from_hd(folder+\"/dendritic_gene_pairs_200.txt\")\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using expression dataframe and gene pairs+labels, create heatmaps of different gene pairs \n",
    "#and their associated label (0 = not correlated, 1 = correlated)\n",
    "X,y=generate_data(expression_df,pairs,y_,s=16,threshold=0.00)\n",
    "\n",
    "X_=X[:20000]\n",
    "y_=y[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that data looks good, i.e. X_ corresponds to matrix corresponding to pixel values of heatmap\n",
    "                                 #y_ correspond to single-value label of 0 or 1\n",
    "print(X_)\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new files containing the relevant training data--both the vectorized\n",
    "with open('X_Data_all.pkl','wb') as f:\n",
    "    pickle.dump(X_, f)\n",
    "\n",
    "with open('y_Data_all.pkl','wb') as f:\n",
    "    pickle.dump(y_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing model architecture and evaluation pipeline\n",
    "\n",
    "Now that we have our training data, the next step is to:\n",
    "\n",
    "1) develop our different neural net architectures\n",
    "\n",
    "2) train our three different models using the training data developed in the section above\n",
    "\n",
    "\n",
    "This section documents all the helper functions we developed in order to do this.\n",
    "\n",
    "\n",
    "References for this section:\n",
    "https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statement\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# demonstration of calculating metrics for a neural network model using sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "import pickle\n",
    "\n",
    "#from keras.layers import Input, Dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import InputLayer\n",
    "\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1 ARCHITECTURE\n",
    "def define_model_1(num_classes=2,shape_input=(150, 150, 1)):\n",
    "\n",
    "\n",
    "    inputs = Input(shape=shape_input)\n",
    "\n",
    "    x=Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu')(inputs)\n",
    "    x=Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(128, activation='relu')(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    predictions=Dense(num_classes, activation='softmax',name='final_output')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    print(\"model is defined\")    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2 ARCHITECTURE\n",
    "def define_model_2(num_classes=2,shape_input=(150, 150, 1)):\n",
    "\n",
    "\n",
    "    inputs = Input(shape=shape_input)\n",
    "\n",
    "    x=Conv2D(16, kernel_size=(3, 3),activation='relu')(inputs)\n",
    "\n",
    "    x=Conv2D(32, (3, 3), activation='relu')(x)\n",
    "   \n",
    "    x=Conv2D(64, (9, 9), activation='relu')(x)\n",
    "  \n",
    "    x=Conv2D(128, (17, 17), activation='relu')(x)    \n",
    "    x=MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x=Dropout(0.25)(x)    \n",
    "\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(128, activation='relu')(x)\n",
    "    x=Dropout(0.5)(x)\n",
    "    predictions=Dense(num_classes, activation='softmax',name='final_output')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 3 ARCHITECTURE\n",
    "def define_model_3(num_classes,shape_input=(32, 32, 1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',input_shape=shape_input))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    print(\"model is defined\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train,x_test, y_test, batch_size=128,epochs=30,l_r=0.05,beta1=0.9,beta2=0.999):\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=l_r,beta_1=beta1,beta_2=beta2),\n",
    "                  metrics=[metrics.categorical_accuracy,metrics.mae])\n",
    "    print(\"training the model\")\n",
    "    history=model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    ##create plot showing accuracy over epochs on both training and test set\n",
    "    plt.plot(history.history['categorical_accuracy']) #instead of 'acc'\n",
    "    plt.plot(history.history['val_categorical_accuracy'])#instead of 'val_acc'\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    ##evaluation metrics of model at end\n",
    "    print(\"done training the model\")\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data prepped above into training and test sets\n",
    "def prepare_data(X,y,test_size=0.05):        \n",
    "    \n",
    "    img_rows, img_cols = 150, 150\n",
    "    \n",
    "    # the data, split between train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_test_sklearn=y_test\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, 2)\n",
    "    y_test = keras.utils.to_categorical(y_test, 2)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test,y_test_sklearn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns even more possible evaluation metrics, just to be thorough\n",
    "def get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn):\n",
    "    \n",
    "    yhat_probs = model.predict(x_test, verbose=0)\n",
    "    print(yhat_probs)\n",
    "    \n",
    "    \n",
    "    def convert_2_sk_fommat(yhat_probs,y_test_sklearn):\n",
    "        out=[]\n",
    "        for i in range(0,len(yhat_probs)):\n",
    "            if y_test_sklearn[i]==1:\n",
    "                \n",
    "                out.append(yhat_probs[0][1])\n",
    "            else:\n",
    "                out.append(yhat_probs[0][0])\n",
    "        return out   \n",
    "                \n",
    "    \n",
    "    # predict crisp classes for test set\n",
    "    yhat_classes = yhat_probs.argmax(axis=1)    \n",
    "    print(yhat_classes)\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test_sklearn, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test_sklearn, yhat_classes)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test_sklearn, yhat_classes)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test_sklearn, yhat_classes)\n",
    "    print('F1 score: %f' % f1)    \n",
    "\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test_sklearn, yhat_classes)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    yhat_probs_sklearn=convert_2_sk_fommat(yhat_probs,y_test_sklearn)\n",
    "    \n",
    "    auc = roc_auc_score(y_test_sklearn, yhat_probs_sklearn)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    print(\"the confusion matrix : \")\n",
    "    matrix = confusion_matrix(y_test_sklearn, yhat_classes)\n",
    "    print(matrix)\n",
    "    \n",
    "    return yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_loss_accuracy(history):\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Loss')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    # plot accuracy during training\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Accuracy')\n",
    "    pyplot.plot(history.history['categorical_accuracy'], label='train')\n",
    "    pyplot.plot(history.history['val_categorical_accuracy'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training our model, we noted some that our heavily unbalanced dataset was causing our neural nets to be trained to always output the majority class. To combat this, we created a downsampling helper function to correct for data imbalance.\n",
    "\n",
    "Reference for helper function:\n",
    "https://chrisalbon.com/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_downsampling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder = '/Users/sierra.lear/Documents/GitHub/deep_cell_personal'\n",
    "\n",
    "with open(new_folder+'/X_Data_all.pkl','rb') as f:\n",
    "    X_ = pickle.load(f)\n",
    "    print(X_.shape)\n",
    "\n",
    "with open(new_folder+'/y_Data_all.pkl','rb') as f:\n",
    "    y_ = pickle.load(f)\n",
    "    print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_equals_0 = np.where(y_ == 0)[0]\n",
    "y_equals_1 = np.where(y_ == 1)[0]\n",
    "\n",
    "print(\"Number of total data points:\", len(y_))\n",
    "print(\"Number of data points with label 0:\",len(y_equals_0))\n",
    "print(\"Number of data points with label 1:\",len(y_equals_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_data(X_, y_):\n",
    "    np.random.seed(8)\n",
    "    #Find the indices for the 0 and 1 class\n",
    "    i_0 = np.where(y_ == 0)[0]\n",
    "    i_1 = np.where(y_ == 1)[0]\n",
    "    #count number of observations in each class\n",
    "    n_0 = len(i_0)\n",
    "    n_1 = len(i_1)\n",
    "    #for every observation in class 0 (small class size), randomly sample from class 1\n",
    "    i_1_downsample = np.random.choice(i_1, size = n_0, replace=False)\n",
    "    #recreate X_ and y_ with only downsampled values\n",
    "    X_downsampled = np.concatenate((X_[i_0,:,:], X_[i_1_downsample,:,:]), axis=0)\n",
    "    y_downsampled = np.hstack((y_[i_0], y_[i_1_downsample]))\n",
    "    \n",
    "    return X_downsampled, y_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_down, y_down = downsampling_data(X_, y_)\n",
    "print(\"Number of total data points:\", len(y_down))\n",
    "print(\"Number of data points with label 0:\",len(np.where(y_down == 0)[0]))\n",
    "print(\"Number of data points with label 1:\",len(np.where(y_down == 1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "new_folder = '/Users/sierra.lear/Documents/GitHub/deep_cell_personal'\n",
    "\n",
    "with open(new_folder+'/X_Data_all.pkl','rb') as f:\n",
    "    X_ = pickle.load(f)\n",
    "    print(X_.shape)\n",
    "\n",
    "with open(new_folder+'/y_Data_all.pkl','rb') as f:\n",
    "    y_ = pickle.load(f)\n",
    "    print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsample the data to create a balanced dataset\n",
    "X_down, y_down = downsampling_data(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the training and test sets from the balanced downsampled dataset\n",
    "x_train, y_train, x_test, y_test,y_test_sklearn = prepare_data(X_down,y_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the standard pipeline we will be using to train the models\n",
    "\"\"\"\n",
    "\n",
    "##(1) define the model: have define_model_1, define_model_2, and define_model 3 to choose from \n",
    "#model=define_model_3(num_classes=2,shape_input=(150, 150, 1))\n",
    "\n",
    "##(2) fit the model   -- we can also play with different hyperparameters here\n",
    "#history=train_model(model, x_train, y_train,x_test, y_test, batch_size=32,epochs=2,l_r=0.005,beta1=0.9,beta2=0.999)\n",
    "\n",
    "##(3) plot history :\n",
    "#plot_loss_accuracy(history)\n",
    "\n",
    "##(4) get accuracy,  f1 score, confusion matrix, etc.   \n",
    "#yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy=get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "to plot ROC :\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first pass, I will use all the data, 2 epochs, a batch size of 32 to make sure everything works. Afterwards, I will be changing both learning rate and epoch size as my two hyperparameters of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) define the model: have define_model_1, define_model_2, and define_model 3 to choose from \n",
    "model=define_model_1(num_classes=2,shape_input=(150, 150, 1))\n",
    "\n",
    "#(2) fit the model   -- we can also play with different hyperparameters here\n",
    "history=train_model(model, x_train, y_train,x_test, y_test, batch_size=32,epochs=2,l_r=0.005,beta1=0.9,beta2=0.999)\n",
    "\n",
    "#(3) plot history :\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "#(4) get accuracy,  f1 score, confusion matrix, etc.   \n",
    "yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy=get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline \n",
    "\n",
    "#(1) load the data    \n",
    "\n",
    "\n",
    "\n",
    "with open('X_Data_all.pkl','rb') as f:\n",
    "    X_ = pickle.load(f)\n",
    "    print(X_.shape)\n",
    "\n",
    "with open('y_Data_all.pkl','rb') as f:\n",
    "    y_ = pickle.load(f)\n",
    "    print(y_.shape)\n",
    "    \n",
    "X = X_[:100]\n",
    "Y = y_[:100]\n",
    "\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test,y_test_sklearn = prepare_data(X,Y)\n",
    "\n",
    "\n",
    "#(2) define the model   \n",
    "\n",
    "\n",
    "model=define_model_1(num_classes=2,shape_input=(150, 150, 1))\n",
    "\n",
    "\n",
    "\n",
    "# #(3) fit the model   \n",
    "\n",
    "# history=train_model(model, x_train, y_train,x_test, y_test, batch_size=32,epochs=2,l_r=0.005,beta1=0.9,beta2=0.999)\n",
    "\n",
    "\n",
    "# # (4) plot history :\n",
    "\n",
    "# plot_loss_accuracy(history)\n",
    "\n",
    "\n",
    "\n",
    "# #(5) get accurcy,  f1 score, confusion matrix, etc   \n",
    "\n",
    "\n",
    "\n",
    "# yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy=get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=define_model_2(num_classes=2,shape_input=(150, 150, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#(3) fit the model   \n",
    "\n",
    "history=train_model(model, x_train, y_train,x_test, y_test, batch_size=32,epochs=2,l_r=0.005,beta1=0.9,beta2=0.999)\n",
    "\n",
    "\n",
    "# (4) plot history :\n",
    "\n",
    "plot_loss_accuracy(history)\n",
    "\n",
    "\n",
    "\n",
    "#(5) get accurcy,  f1 score, confusion matrix, etc   \n",
    "\n",
    "\n",
    "\n",
    "yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy=get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs_sklearn, y_test_sklearn,matrix,f1,recall,precision,accuracy=get_scores_confusion_matrix_etc(model,x_test,y_test_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
